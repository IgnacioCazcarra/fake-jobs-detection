{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Fake-job-prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+\n",
      "|job_id|           title|        location|department|salary_range|     company_profile|         description|        requirements|benefits|telecommuting|has_company_logo|has_questions|employment_type|required_experience|required_education|industry| function|fraudulent|\n",
      "+------+----------------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+\n",
      "|     1|Marketing Intern|US, NY, New York| Marketing|        null|We're Food52, and...|Food52, a fast-gr...|Experience with c...|    null|            0|               1|            0|          Other|         Internship|              null|    null|Marketing|         0|\n",
      "+------+----------------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"escape\",\"\\\"\").option(\"quote\",\"\\\"\").load(\"fake_job_postings.csv\", sep=\",\")\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('job_id', 'string'),\n",
       " ('title', 'string'),\n",
       " ('location', 'string'),\n",
       " ('department', 'string'),\n",
       " ('salary_range', 'string'),\n",
       " ('company_profile', 'string'),\n",
       " ('description', 'string'),\n",
       " ('requirements', 'string'),\n",
       " ('benefits', 'string'),\n",
       " ('telecommuting', 'string'),\n",
       " ('has_company_logo', 'string'),\n",
       " ('has_questions', 'string'),\n",
       " ('employment_type', 'string'),\n",
       " ('required_experience', 'string'),\n",
       " ('required_education', 'string'),\n",
       " ('industry', 'string'),\n",
       " ('function', 'string'),\n",
       " ('fraudulent', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+\n",
      "|job_id|           title|        location|department|salary_range|     company_profile|         description|        requirements|benefits|telecommuting|has_company_logo|has_questions|employment_type|required_experience|required_education|industry| function|fraudulent|\n",
      "+------+----------------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+\n",
      "|     1|marketing intern|us, ny, new york| marketing|        null|we're food52, and...|food52, a fast-gr...|experience with c...|    null|            0|               1|            0|          other|         internship|              null|    null|marketing|         0|\n",
      "+------+----------------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    df = df.withColumn(col, F.lower(col))\n",
    "    \n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We divide the 'location' column into 'country' and 'city'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def get_country(location):\n",
    "    if location is None:\n",
    "        return None\n",
    "    else:\n",
    "        location_list = location.strip().split(sep = \",\")\n",
    "        return location_list[0]\n",
    "\n",
    "def get_city(location):\n",
    "    if location is None:\n",
    "        return None\n",
    "    else:\n",
    "        location_list = location.strip().split(sep = \",\")\n",
    "        if(len(location_list)>1 and not location_list[1].isspace()):\n",
    "            return location_list[1]\n",
    "        else: \n",
    "            return None\n",
    "\n",
    "udf_country = F.udf(get_country, StringType())\n",
    "udf_city = F.udf(get_city, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+-------+----+\n",
      "|job_id|           title|department|salary_range|     company_profile|         description|        requirements|benefits|telecommuting|has_company_logo|has_questions|employment_type|required_experience|required_education|industry| function|fraudulent|country|city|\n",
      "+------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+-------+----+\n",
      "|     1|marketing intern| marketing|        null|we're food52, and...|food52, a fast-gr...|experience with c...|    null|            0|               1|            0|          other|         internship|              null|    null|marketing|         0|     us|  ny|\n",
      "+------+----------------+----------+------------+--------------------+--------------------+--------------------+--------+-------------+----------------+-------------+---------------+-------------------+------------------+--------+---------+----------+-------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"country\", udf_country(F.col('location')))\n",
    "df = df.withColumn(\"city\", udf_city(F.col('location')))\n",
    "\n",
    "#Due to the poor form of the location column, country and state are the only useful columns we can get.\n",
    "df = df.drop(F.col('location'))\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the salary_range column may vary because of different currencies, we get the currency for each country and we convert it to USD. If its not available we leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"country\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"https://unece.org/fileadmin/DAM/cefact/recommendations/bkup_htm/cocucod.htm\"\n",
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes = []\n",
    "currency_codes = []\n",
    "\n",
    "info = soup.find_all(\"font\", attrs={'color':'#000000','size':'1','face':'Verdana'})\n",
    "\n",
    "country_codes = [info[x].text.lower() for x in range(1526) if len(info[x].text)==2]\n",
    "\n",
    "currency_codes = [info[x].text.lower() for x in range(1526) \n",
    "                  if len(info[x].text)==3 \n",
    "                  and not info[x].text.isnumeric() \n",
    "                  and info[x].text.isupper()]\n",
    "\n",
    "#There are three countries with no official currency in this table, so we have to drop them.\n",
    "remove_list = ['gs','ps','aq']\n",
    "\n",
    "country_codes = [x for x in country_codes if x not in remove_list]\n",
    "\n",
    "CC = list(zip(country_codes,currency_codes))\n",
    "\n",
    "len(CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currencycode(country):\n",
    "    idx = 0\n",
    "    for i in range(len(CC)):\n",
    "        if CC[i][0]==country:\n",
    "            return CC[idx][1]\n",
    "        idx+=1\n",
    "    return None       \n",
    "    \n",
    "\n",
    "udf_currency = F.udf(get_currencycode, StringType())\n",
    "\n",
    "\n",
    "df = df.withColumn('salary_currency', udf_currency(F.col('country')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+-------------+----------------+-------------+---------------+-------------------+------------------+--------------------+--------------------+----------+-------+----+---------------+----------+----------+\n",
      "|job_id|               title|department|     company_profile|         description|        requirements|            benefits|telecommuting|has_company_logo|has_questions|employment_type|required_experience|required_education|            industry|            function|fraudulent|country|city|salary_currency|salary_min|salary_max|\n",
      "+------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+-------------+----------------+-------------+---------------+-------------------+------------------+--------------------+--------------------+----------+-------+----+---------------+----------+----------+\n",
      "|     1|    marketing intern| marketing|we're food52, and...|food52, a fast-gr...|experience with c...|                null|            0|               1|            0|          other|         internship|              null|                null|           marketing|         0|     us|  ny|            usd|      null|      null|\n",
      "|     2|customer service ...|   success|90 seconds, the w...|organised - focus...|what we expect fr...|what you will get...|            0|               1|            0|      full-time|     not applicable|              null|marketing and adv...|    customer service|         0|     nz|null|            nzd|      null|      null|\n",
      "|     3|commissioning mac...|      null|valor services pr...|our client, locat...|implement pre-com...|                null|            0|               1|            0|           null|               null|              null|                null|                null|         0|     us|  ia|            usd|      null|      null|\n",
      "|     4|account executive...|     sales|our passion for i...|the company: esri...|education:Â bachel...|our culture is an...|            0|               1|            0|      full-time|   mid-senior level| bachelor's degree|   computer software|               sales|         0|     us|  dc|            usd|      null|      null|\n",
      "|     5| bill review manager|      null|spotsource soluti...|job title: itemiz...|qualifications:rn...|full benefits off...|            0|               1|            1|      full-time|   mid-senior level| bachelor's degree|hospital & health...|health care provider|         0|     us|  fl|            usd|      null|      null|\n",
      "+------+--------------------+----------+--------------------+--------------------+--------------------+--------------------+-------------+----------------+-------------+---------------+-------------------+------------------+--------------------+--------------------+----------+-------+----+---------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def get_minRange(salary):\n",
    "    if salary is None:\n",
    "        return None\n",
    "    salary_range = salary.strip().split(sep = \"-\")\n",
    "    try: \n",
    "        r = int(salary_range[0])\n",
    "        return r \n",
    "    except ValueError: \n",
    "        return None\n",
    "\n",
    "def get_maxRange(salary):\n",
    "    if salary is None:\n",
    "        return None\n",
    "    salary_range = salary.strip().split(sep = \"-\")\n",
    "    try:\n",
    "        if(len(salary_range)==1):\n",
    "            r = int(salary_range[0])    \n",
    "        else:\n",
    "            r = int(salary_range[1])\n",
    "        return r\n",
    "    except ValueError: \n",
    "        return None\n",
    "\n",
    "udf_minRange = F.udf(get_minRange, IntegerType())\n",
    "udf_maxRange = F.udf(get_maxRange, IntegerType())\n",
    "\n",
    "df = df.withColumn('salary_min', udf_minRange(F.col('salary_range')))\n",
    "df = df.withColumn('salary_max', udf_maxRange(F.col('salary_range')))\n",
    "\n",
    "df = df.drop(F.col('salary_range'))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_CURRENCIES = 'https://api.exchangerate-api.com/v4/latest/USD'\n",
    "\n",
    "curr = requests.get(URL_CURRENCIES).json()\n",
    "curr = curr['rates']\n",
    "\n",
    "curr =  {k.lower(): v for k, v in curr.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usd(number, currency):\n",
    "    if number is None or currency is None or currency not in list(curr.keys()):\n",
    "        return None\n",
    "    else:\n",
    "        return int(number/curr[currency])\n",
    "\n",
    "udf_usd = F.udf(get_usd, IntegerType())\n",
    "\n",
    "df = df.withColumn('salary_min', udf_usd(F.col('salary_min'),F.col('salary_currency')))\n",
    "df = df.withColumn('salary_max', udf_usd(F.col('salary_max'),F.col('salary_currency')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "def re_links(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z-#]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" \", text)\n",
    "    #Remove words with more than 20 length, because they are more likely to be a link that we didnt filter for some reason.\n",
    "    return  re.sub(r'\\b\\w{20,}\\b', ' ', text)\n",
    "\n",
    "def re_xa0(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return text.replace('\\\\xa0', ' ')\n",
    "\n",
    "def re_punctuations(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return re.sub(\"[^0-9-a-z']\", \" \" , text)\n",
    "\n",
    "def re_space(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re_links(text)\n",
    "    text = re_xa0(text)\n",
    "    text = re_punctuations(text)\n",
    "    text = re_space(text)\n",
    "    return text\n",
    "\n",
    "udf_cleaning = F.udf(text_cleaning, StringType())\n",
    "    \n",
    "text_columns = ['title','department','company_profile','description','requirements','benefits','industry']\n",
    "\n",
    "for col in text_columns:\n",
    "    df = df.withColumn(col, udf_cleaning(F.col(col)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " title:10944\n",
      " department:1268\n",
      " company_profile:1706\n",
      " description:14514\n",
      " requirements:11844\n",
      " benefits:5891\n"
     ]
    }
   ],
   "source": [
    "#we process industry column to remove any kind of undesired punctuations but it only has 132 distinct variables \n",
    "#so it might be better to one hot encode it\n",
    "text_columns.remove('industry')\n",
    "\n",
    "for col in text_columns:\n",
    "    print(f' {col}:{df.select(col).distinct().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------------+-------------+---------------+-------------------+------------------+--------------------+--------------------+----------+-------+----+---------------+----------+----------+--------------------+----------------------+---------------------------+-----------------------+------------------------+--------------------+\n",
      "|job_id|telecommuting|has_company_logo|has_questions|employment_type|required_experience|required_education|            industry|            function|fraudulent|country|city|salary_currency|salary_min|salary_max|   title_feature_vec|department_feature_vec|company_profile_feature_vec|description_feature_vec|requirements_feature_vec|benefits_feature_vec|\n",
      "+------+-------------+----------------+-------------+---------------+-------------------+------------------+--------------------+--------------------+----------+-------+----+---------------+----------+----------+--------------------+----------------------+---------------------------+-----------------------+------------------------+--------------------+\n",
      "|     1|            0|               1|            0|          other|         internship|              null|                null|           marketing|         0|     us|  ny|            usd|      null|      null|(20,[2,16],[1.786...|  (20,[2],[2.888751...|       (20,[0,1,2,3,4,5,...|   (20,[0,1,2,3,4,5,...|    (20,[0,1,2,3,4,5,...|(20,[12],[0.18621...|\n",
      "|     2|            0|               1|            0|      full-time|     not applicable|              null|marketing and adv...|    customer service|         0|     nz|null|            nzd|      null|      null|(20,[7,12,13,14],...|  (20,[8],[3.872600...|       (20,[0,1,2,3,4,5,...|   (20,[0,1,2,3,4,5,...|    (20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|     3|            0|               1|            0|           null|               null|              null|                null|                null|         0|     us|  ia|            usd|      null|      null|(20,[5,12,13,18],...|  (20,[12],[0.41056...|       (20,[0,1,2,3,4,5,...|   (20,[0,2,3,4,5,6,...|    (20,[0,1,2,3,4,5,...|(20,[12],[0.18621...|\n",
      "|     4|            0|               1|            0|      full-time|   mid-senior level| bachelor's degree|   computer software|               sales|         0|     us|  dc|            usd|      null|      null|(20,[2,6,7,12,18]...|  (20,[7],[2.714840...|       (20,[0,1,2,3,4,5,...|   (20,[0,1,2,3,4,5,...|    (20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|     5|            0|               1|            1|      full-time|   mid-senior level| bachelor's degree|hospital health care|health care provider|         0|     us|  fl|            usd|      null|      null|(20,[0,12,15],[1....|  (20,[12],[0.41056...|       (20,[0,1,2,3,4,5,...|   (20,[0,1,2,3,4,5,...|    (20,[0,1,2,3,4,5,...|(20,[13,17],[0.79...|\n",
      "+------+-------------+----------------+-------------+---------------+-------------------+------------------+--------------------+--------------------+----------+-------+----+---------------+----------+----------+--------------------+----------------------+---------------------------+-----------------------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "df = df.fillna(\"\", subset=text_columns)\n",
    "\n",
    "for col in text_columns:\n",
    "    tokenizer = Tokenizer(inputCol=col, outputCol=f\"{col}_words\")\n",
    "    wordsData = tokenizer.transform(df)\n",
    "\n",
    "    hashingTF = HashingTF(inputCol=f\"{col}_words\", outputCol=f\"{col}_raw\", numFeatures=20)\n",
    "    featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "    idf = IDF(inputCol=f\"{col}_raw\", outputCol=f\"{col}_feature_vec\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    df = rescaledData\n",
    "    df = df.drop(col)\n",
    "    df = df.drop(f\"{col}_words\")\n",
    "    df = df.drop(f\"{col}_raw\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply One-hot-encoding to some columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " country:91\n",
      " city:325\n",
      " salary_currency:78\n",
      " required_experience:8\n",
      " required_education:14\n",
      " employment_type:6\n",
      " industry:132\n",
      " function:38\n"
     ]
    }
   ],
   "source": [
    "ohe_cols = ['country','city','salary_currency','required_experience','required_education','employment_type','industry','function']\n",
    "\n",
    "for col in ohe_cols:\n",
    "    print(f' {col}:{df.select(col).distinct().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "for col in ohe_cols:\n",
    "    stringIndexer = StringIndexer(inputCol=col, outputCol= f\"{col}_index\").setHandleInvalid(\"keep\")\n",
    "    model = stringIndexer.fit(df)\n",
    "    indexed = model.transform(df)\n",
    "    encoder = OneHotEncoder(dropLast=False, inputCol=f\"{col}_index\", outputCol=f\"{col}_vec\")\n",
    "    encoded = encoder.fit(indexed).transform(indexed)\n",
    "    df = encoded\n",
    "    df = df.drop(F.col(f\"{col}_index\"))\n",
    "    df = df.drop(F.col(col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------------+-------------+----------+----------+----------+--------------------+----------------------+---------------------------+-----------------------+------------------------+--------------------+--------------+-----------------+-------------------+-----------------------+----------------------+-------------------+-----------------+---------------+\n",
      "|job_id|telecommuting|has_company_logo|has_questions|fraudulent|salary_min|salary_max|   title_feature_vec|department_feature_vec|company_profile_feature_vec|description_feature_vec|requirements_feature_vec|benefits_feature_vec|   country_vec|         city_vec|salary_currency_vec|required_experience_vec|required_education_vec|employment_type_vec|     industry_vec|   function_vec|\n",
      "+------+-------------+----------------+-------------+----------+----------+----------+--------------------+----------------------+---------------------------+-----------------------+------------------------+--------------------+--------------+-----------------+-------------------+-----------------------+----------------------+-------------------+-----------------+---------------+\n",
      "|     1|            0|               1|            0|         0|      null|      null|(20,[2,16],[1.786...|  (20,[2],[2.888751...|       (20,[0,1,2,3,4,5,...|   (20,[0,1,2,3,4,5,...|    (20,[0,1,2,3,4,5,...|(20,[12],[0.18621...|(91,[0],[1.0])|  (325,[1],[1.0])|     (78,[0],[1.0])|          (8,[5],[1.0])|       (14,[13],[1.0])|      (6,[4],[1.0])|(132,[131],[1.0])| (38,[4],[1.0])|\n",
      "|     2|            0|               1|            0|         0|      null|      null|(20,[7,12,13,14],...|  (20,[8],[3.872600...|       (20,[0,1,2,3,4,5,...|   (20,[0,1,2,3,4,5,...|    (20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|(91,[5],[1.0])|(325,[324],[1.0])|     (78,[4],[1.0])|          (8,[3],[1.0])|       (14,[13],[1.0])|      (6,[0],[1.0])|  (132,[3],[1.0])| (38,[3],[1.0])|\n",
      "|     3|            0|               1|            0|         0|      null|      null|(20,[5,12,13,18],...|  (20,[12],[0.41056...|       (20,[0,1,2,3,4,5,...|   (20,[0,2,3,4,5,6,...|    (20,[0,1,2,3,4,5,...|(20,[12],[0.18621...|(91,[0],[1.0])| (325,[35],[1.0])|     (78,[0],[1.0])|          (8,[7],[1.0])|       (14,[13],[1.0])|      (6,[5],[1.0])|(132,[131],[1.0])|(38,[37],[1.0])|\n",
      "+------+-------------+----------------+-------------+----------+----------+----------+--------------------+----------------------+---------------------------+-----------------------+------------------------+--------------------+--------------+-----------------+-------------------+-----------------------+----------------------+-------------------+-----------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 4 columns that are string so we need to convert them into proper type \n",
    "df = df.drop('job_id')\n",
    "\n",
    "bool_cols = ['telecommuting','has_company_logo','has_questions','fraudulent']\n",
    "\n",
    "for c in bool_cols:\n",
    "    df = df.withColumn(c, F.col(c).cast('boolean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove('fraudulent')\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cols,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.setHandleInvalid(\"keep\").transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = output.select(F.col('features'), F.col('fraudulent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(817,[1,3,4,7,21,...|false|\n",
      "|(817,[1,3,4,12,17...|false|\n",
      "|(817,[1,3,4,10,17...|false|\n",
      "|(817,[1,3,4,7,11,...|false|\n",
      "|(817,[1,2,3,4,5,1...|false|\n",
      "|(817,[3,4,6,16,37...|false|\n",
      "|(817,[1,2,3,4,9,1...|false|\n",
      "|(817,[1,2,3,4,17,...|false|\n",
      "|(817,[1,2,3,4,10,...|false|\n",
      "|(817,[1,3,4,5,16,...|false|\n",
      "|(817,[3,4,6,7,8,1...|false|\n",
      "|(817,[1,3,4,5,10,...|false|\n",
      "|(817,[1,3,4,6,7,1...|false|\n",
      "|(817,[1,2,3,4,22,...|false|\n",
      "|(817,[1,3,4,7,11,...|false|\n",
      "|(817,[1,2,3,4,12,...|false|\n",
      "|(817,[1,3,4,11,18...|false|\n",
      "|(817,[1,2,3,4,10,...|false|\n",
      "|(817,[1,3,4,5,11,...|false|\n",
      "|(817,[3,4,5,9,17,...|false|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('fraudulent', \"label\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataframe and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('label', F.col('label').cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = df.randomSplit([0.80,0.20], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.maxDepth, [10, 20, 30]).addGrid(rf.numTrees, [10, 25, 50, 100]).build()\n",
    "    \n",
    "tvs = TrainValidationSplit(estimator=rf,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=BinaryClassificationEvaluator(metricName='areaUnderPR'),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(817,[0,1,2,3,4,5...|    0|[85.9833015820015...|[0.85983301582001...|         0|\n",
      "|(817,[0,1,2,3,4,5...|    0|[99.9013318062132...|[0.99901331806213...|         0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions = model.transform(test_data)\n",
    "rf_predictions = rf_predictions.withColumn('prediction', F.col('prediction').cast('integer'))\n",
    "\n",
    "\n",
    "rf_predictions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8913557376322941\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName='areaUnderPR')\n",
    "\n",
    "print(evaluator.evaluate(rf_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
